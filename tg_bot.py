from dotenv import dotenv_values
import os
import re
import nltk
import pickle
import random
import pymorphy3
import numpy as np
import tensorflow as tf
from telegram import Update
from nltk.corpus import stopwords
from telegram.ext import Application, CommandHandler, MessageHandler, filters, ContextTypes
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.sequence import pad_sequences


# –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
CONFIG = dotenv_values(".env")
TOKEN = CONFIG.get('TOKEN')  # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–∫–µ–Ω –±–µ–∑–æ–ø–∞—Å–Ω—ã–º —Å–ø–æ—Å–æ–±–æ–º

if not TOKEN:
    raise ValueError("–ù–µ –Ω–∞–π–¥–µ–Ω TOKEN –≤ .env —Ñ–∞–π–ª–µ")


class NLPProcessor:
    def __init__(self):
        self.morph = pymorphy3.MorphAnalyzer()
        self.stopwords = self._load_stopwords()
    
    def _load_stopwords(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç–æ–ø-—Å–ª–æ–≤ —Å —Ä–µ–∑–µ—Ä–≤–Ω—ã–º –≤–∞—Ä–∏–∞–Ω—Ç–æ–º"""
        try:
            nltk.data.find('corpora/stopwords')
        except:
            try:
                nltk.download('stopwords', quiet=True)
            except:
                pass
        
        try:
            stops = set(stopwords.words('russian'))
        except:
            # Fallback-—Å–ø–∏—Å–æ–∫
            stops = {
                '–∏', '–≤', '–≤–æ', '–Ω–µ', '—á—Ç–æ', '–æ–Ω', '–Ω–∞', '—è', '—Å', '—Å–æ', '–∫–∞–∫', '–∞',
                '—Ç–æ', '–≤—Å–µ', '–æ–Ω–∞', '—Ç–∞–∫', '–µ–≥–æ', '–Ω–æ', '–¥–∞', '—Ç—ã', '–∫', '—É', '–∂–µ'
            }
        
        # –î–æ–ø–æ–ª–Ω—è–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–º–∏ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞–º–∏
        stops.update({'—ç—Ç–æ', '–≤–æ—Ç', '–Ω—É', '–¥–∞–≤–∞–π', '–ª–∞–¥–Ω–æ', '–∑–Ω–∞—á–∏—Ç'})
        return stops

    def preprocess_text(self, text):
        """–ü–æ–ª–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞: —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è + –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è + —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è"""
        words = re.findall(r'[–∞-—è—ëa-z\-]+', text.lower())
        processed_words = []
        
        for word in words:
            try:
                lemma = self.morph.parse(word)[0].normal_form
                if lemma not in self.stopwords:
                    processed_words.append(lemma)
            except:
                continue
                
        return processed_words

class TelegramLSTMBot:
    def __init__(self, model_path='./dist/lstm_chatbot.keras', vocab_path='./dist/lstm_chatbot_vocab.pkl'):
        self.model = None
        self.tokenizer = None
        self.word_index = None
        self.index_word = None
        self.seq_length = 20
        self.temperature = 0.7
        self.max_words = 5000
        self.nlp = NLPProcessor()
        
        self.load_model(model_path, vocab_path)
    
    def load_model(self, model_path, vocab_path):
        if os.path.exists(model_path) and os.path.exists(vocab_path):
            self.model = load_model(model_path)
            with open(vocab_path, 'rb') as f:
                vocab_data = pickle.load(f)
                self.tokenizer = vocab_data['tokenizer']
                self.word_index = vocab_data['word_index']
                self.index_word = vocab_data['index_word']
            print("–ú–æ–¥–µ–ª—å –∏ —Å–ª–æ–≤–∞—Ä—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
        else:
            raise FileNotFoundError("–§–∞–π–ª—ã –º–æ–¥–µ–ª–∏ –∏–ª–∏ —Å–ª–æ–≤–∞—Ä—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
    
    def preprocess_input(self, text):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–≤–æ–¥–∞ —Å –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–µ–π"""
        words = self.nlp.preprocess_text(text)
        
        # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ –¥–ª–∏–Ω—ã –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        if len(words) > self.seq_length:
            words = words[-self.seq_length:]
        elif len(words) < self.seq_length:
            words = [''] * (self.seq_length - len(words)) + words
            
        return words
    
    def generate_response(self, input_text, max_length=20, min_prob=0.05):
        if not self.model or not self.tokenizer:
            return "–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞"
            
        seed_words = self.preprocess_input(input_text)
        if not seed_words:
            return "–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –≤–∞—à –∑–∞–ø—Ä–æ—Å"
            
        generated = []
        for _ in range(max_length):
            sequence = self.tokenizer.texts_to_sequences([' '.join(seed_words)])
            if not sequence or not sequence[0]:
                break
                
            sequence = pad_sequences(sequence, maxlen=self.seq_length, padding='post')
            
            # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–æ–¥–µ–ª–∏
            preds = self.model.predict(sequence, verbose=0)[0]
            preds = np.clip(preds, 1e-10, 1.0)  # –ó–∞—â–∏—Ç–∞ –æ—Ç –Ω—É–ª–µ–≤—ã—Ö –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
            
            # –í—ã–±–∏—Ä–∞–µ–º —Å–ª–µ–¥—É—é—â–µ–µ —Å–ª–æ–≤–æ –∏ –µ–≥–æ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å
            next_idx, prob = self._sample(preds, self.temperature)
            next_word = self.index_word.get(next_idx, "")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É—Å–ª–æ–≤–∏–µ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –ø–æ –Ω–∏–∑–∫–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
            if prob < min_prob and len(generated) > 2:  # –ù–µ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è —Å–ª–∏—à–∫–æ–º —Ä–∞–Ω–æ
                break
                
            if not next_word or next_word == "<OOV>":
                break
                
            generated.append(next_word)
            seed_words.append(next_word)
            seed_words = seed_words[1:]
            
            # –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–æ –∑–Ω–∞–∫–∞–º –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
            if next_word in ['.', '?', '!'] and len(generated) > 2:
                break
        
        # –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ç–≤–µ—Ç–∞
        response = ' '.join(generated).capitalize()
        if response and not response.endswith(('.','!','?')):
            response += '.' if len(response) > 5 else ""
            
        return response if response else "–ù–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç"

    def _sample(self, preds, temperature):
        preds = np.asarray(preds).astype('float64')
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–Ω–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
        if temperature > 0:
            preds = np.log(preds + 1e-10) / temperature  # –ó–∞—â–∏—Ç–∞ –æ—Ç log(0)
            exp_preds = np.exp(preds - np.max(preds))  # –ß–∏—Å–ª–µ–Ω–Ω–æ —É—Å—Ç–æ–π—á–∏–≤—ã–π –≤–∞—Ä–∏–∞–Ω—Ç
            preds = exp_preds / np.sum(exp_preds)
        
        # –í—ã–±–∏—Ä–∞–µ–º —Å–ª–æ–≤–æ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –µ–≥–æ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å
        next_idx = np.argmax(preds)
        return next_idx, preds[next_idx]

# Telegram Handlers
async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    help_text = """
üìö –Ø - LSTM-–±–æ—Ç —Å NLP-–æ–±—Ä–∞–±–æ—Ç–∫–æ–π:
‚Ä¢ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è
‚Ä¢ –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Å—Ç–æ–ø-—Å–ª–æ–≤
‚Ä¢ –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏

–ö–æ–º–∞–Ω–¥—ã:
/temp - –∏–∑–º–µ–Ω–∏—Ç—å —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—É (0.1-2.0)
/status - –ø–æ–∫–∞–∑–∞—Ç—å —Ç–µ–∫—É—â–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
"""
    await update.message.reply_text(help_text)

async def set_temperature(update: Update, context: ContextTypes.DEFAULT_TYPE):
    try:
        if not context.args:
            await update.message.reply_text(f"üå° –¢–µ–∫—É—â–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞: {bot.temperature}")
            return
            
        new_temp = float(context.args[0])
        if 0.1 <= new_temp <= 2.0:
            bot.temperature = new_temp
            await update.message.reply_text(f"‚úÖ –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞: {bot.temperature}")
        else:
            await update.message.reply_text("‚ùå –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –º–µ–∂–¥—É 0.1 –∏ 2.0")
    except:
        await update.message.reply_text("‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: /temp 0.7")

async def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user_input = update.message.text
    response = bot.generate_response(user_input)
    await update.message.reply_text(response)

async def status(update: Update, context: ContextTypes.DEFAULT_TYPE):
    status_msg = f"""
‚öôÔ∏è –¢–µ–∫—É—â–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏:
‚Ä¢ –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞: {bot.temperature}
‚Ä¢ NLP: {"–ê–∫—Ç–∏–≤–µ–Ω" if bot.nlp.morph else "–û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–π —Ä–µ–∂–∏–º"}
‚Ä¢ –°—Ç–æ–ø-—Å–ª–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω–æ: {len(bot.nlp.stopwords)}
"""
    await update.message.reply_text(status_msg)

if __name__ == '__main__':
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–æ—Ç–∞
    bot = TelegramLSTMBot()
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ Telegram –±–æ—Ç–∞
    app = Application.builder().token(TOKEN).build()
    
    # –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤
    app.add_handler(CommandHandler("start", start))
    app.add_handler(CommandHandler("temp", set_temperature))
    app.add_handler(CommandHandler("status", status))
    app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_message))
    
    print("–ë–æ—Ç –∑–∞–ø—É—â–µ–Ω —Å NLP-–æ–±—Ä–∞–±–æ—Ç–∫–æ–π...")
    app.run_polling()